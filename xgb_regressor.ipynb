{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Read CSV Files\n",
    "# Load the training and testing datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicated rows.\n"
     ]
    }
   ],
   "source": [
    "# ### Check for Duplicates\n",
    "# Concatenate the training and testing datasets to check for duplicates\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Identify duplicated rows in the combined dataset\n",
    "duplicates = combined_data.duplicated()\n",
    "\n",
    "# Check if any duplicates exist and print the results\n",
    "if duplicates.any():\n",
    "    print(\"There are duplicated rows.\")\n",
    "    print(combined_data[duplicates])  # Show the duplicated rows\n",
    "else:\n",
    "    print(\"There are no duplicated rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Handle Missing Values in 'Item_Weight'\n",
    "# The 'Item_Weight' column has missing values that need to be addressed.\n",
    "# Group by 'Item_Identifier' and calculate descriptive statistics to find a suitable replacement for missing values\n",
    "weight_statistics = train_data.groupby('Item_Identifier').agg(\n",
    "    mean=('Item_Weight', 'mean'),\n",
    "    std=('Item_Weight', 'std'),\n",
    "    min=('Item_Weight', 'min'),\n",
    "    q25=('Item_Weight', lambda x: x.quantile(0.25)),\n",
    "    q50=('Item_Weight', 'median'),\n",
    "    q75=('Item_Weight', lambda x: x.quantile(0.75)),\n",
    "    max=('Item_Weight', 'max')\n",
    ")\n",
    "\n",
    "# ### Remove Irrelevant Records\n",
    "# Identify items with a single record that have a null weight\n",
    "# Since there are only 4 such records in over 8k, they will be removed\n",
    "records_to_remove = ['FDN52', 'FDK57', 'FDE52', 'FDQ60']\n",
    "train_data = train_data[~train_data['Item_Identifier'].isin(records_to_remove)]\n",
    "\n",
    "# ### Fill Missing Values with Means\n",
    "# Replace null values in 'Item_Weight' with the mean weight of each item\n",
    "mean_weight_per_item = train_data.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n",
    "train_data['Item_Weight'] = train_data['Item_Weight'].fillna(mean_weight_per_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Standardize 'Item_Fat_Content' Categories\n",
    "# Unify categories that represent the same concept\n",
    "category_replacements = {\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat',\n",
    "    'reg': 'Regular'\n",
    "}\n",
    "train_data['Item_Fat_Content'] = train_data['Item_Fat_Content'].replace(category_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Consolidate Rare Categories in 'Item_Type'\n",
    "# Replace categories with fewer than 200 records as they may be underrepresented\n",
    "train_data.loc[train_data['Item_Type'].isin(['Starchy Foods', 'Breakfast', 'Seafood']), 'Item_Type'] = 'Others'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
