{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Read CSV Files\n",
    "# Load the training and testing datasets\n",
    "df = pd.read_csv('big_mart_sales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Handle Missing Values in 'Item_Weight'\n",
    "# The 'Item_Weight' column has missing values that need to be addressed.\n",
    "# Group by 'Item_Identifier' and calculate descriptive statistics to find a suitable replacement for missing values\n",
    "weight_statistics = df.groupby('Item_Identifier').agg(\n",
    "    mean=('Item_Weight', 'mean'),\n",
    "    std=('Item_Weight', 'std'),\n",
    "    min=('Item_Weight', 'min'),\n",
    "    q25=('Item_Weight', lambda x: x.quantile(0.25)),\n",
    "    q50=('Item_Weight', 'median'),\n",
    "    q75=('Item_Weight', lambda x: x.quantile(0.75)),\n",
    "    max=('Item_Weight', 'max')\n",
    ")\n",
    "\n",
    "# ### Remove Irrelevant Records\n",
    "# Identify items with a single record that have a null weight\n",
    "# Since there are only 4 such records in over 8k, they will be removed\n",
    "records_to_remove = ['FDN52', 'FDK57', 'FDE52', 'FDQ60']\n",
    "df = df[~df['Item_Identifier'].isin(records_to_remove)]\n",
    "\n",
    "# ### Fill Missing Values with Means\n",
    "# Replace null values in 'Item_Weight' with the mean weight of each item\n",
    "mean_weight_per_item = df.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n",
    "df['Item_Weight'] = df['Item_Weight'].fillna(mean_weight_per_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Standardize 'Item_Fat_Content' Categories\n",
    "# Unify categories that represent the same concept\n",
    "category_replacements = {\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat',\n",
    "    'reg': 'Regular'\n",
    "}\n",
    "df['Item_Fat_Content'] = df['Item_Fat_Content'].replace(category_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Consolidate Rare Categories in 'Item_Type'\n",
    "# Replace categories with fewer than 200 records as they may be underrepresented\n",
    "df.loc[df['Item_Type'].isin(['Starchy Foods', 'Breakfast', 'Seafood']), 'Item_Type'] = 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are items with visibility equal to 0, which seems strange, but I will not modify them.\n",
    "# The item may not be displayed in the store, and these items are unique per outlet, so there isn't a suitable metric to replace this value.\n",
    "\n",
    "# Some outlets don't have a size value, and this will remain null since we have no way of determining the actual size of the outlet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the cleaned data to preserve the raw dataset\n",
    "processed_train_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing numerical features. While XGBoost is robust to different scales, standardization is applied for potential category importance analysis and model tuning.\n",
    "# Standardization is also chosen over normalization because it is less sensitive to outliers.\n",
    "scaler = StandardScaler()\n",
    "processed_train_data[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']] = scaler.fit_transform(\n",
    "    processed_train_data[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns since XGBoost does not natively handle categorical data.\n",
    "# Label Encoding is applied to binary columns like 'Item_Fat_Content' and 'Outlet_Size' to capture their binary nature efficiently.\n",
    "le = LabelEncoder()\n",
    "processed_train_data['Item_Fat_Content'] = le.fit_transform(processed_train_data['Item_Fat_Content'])\n",
    "processed_train_data['Outlet_Size'] = le.fit_transform(processed_train_data['Outlet_Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding is applied to 'Item_Type' to handle multiple categories, avoiding imposing any ordinal relationship between them. XGBoost works well with binary inputs.\n",
    "processed_train_data = pd.get_dummies(processed_train_data, columns=['Item_Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying 'Outlet_Location_Type' by removing the \"Tier\" text and converting to integer.\n",
    "processed_train_data['Outlet_Location_Type'] = processed_train_data['Outlet_Location_Type'].str.replace('Tier ', '', regex=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding is also applied to 'Outlet_Type' to handle multiple categories, ensuring no ordinal relationship is inferred between them.\n",
    "processed_train_data = pd.get_dummies(processed_train_data, columns=['Outlet_Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as categoria previsoras da que será prevista\n",
    "# Removendo Identificadores pois geralmente não tem relação direta com as vendas e removendo a coluna q será prevista\n",
    "x = processed_train_data.drop(['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales'], axis = 1)\n",
    "# Isolando a coluna q será prevista\n",
    "y = processed_train_data['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados em 80% para treino e 20% para teste\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1411.56420\n",
      "[1]\tvalidation_0-rmse:1237.48781\n",
      "[2]\tvalidation_0-rmse:1135.92002\n",
      "[3]\tvalidation_0-rmse:1080.56084\n",
      "[4]\tvalidation_0-rmse:1047.38316\n",
      "[5]\tvalidation_0-rmse:1026.19549\n",
      "[6]\tvalidation_0-rmse:1012.79838\n",
      "[7]\tvalidation_0-rmse:998.81379\n",
      "[8]\tvalidation_0-rmse:987.32075\n",
      "[9]\tvalidation_0-rmse:980.60007\n",
      "[10]\tvalidation_0-rmse:971.95163\n",
      "[11]\tvalidation_0-rmse:958.71601\n",
      "[12]\tvalidation_0-rmse:953.59379\n",
      "[13]\tvalidation_0-rmse:945.28447\n",
      "[14]\tvalidation_0-rmse:938.28884\n",
      "[15]\tvalidation_0-rmse:929.63055\n",
      "[16]\tvalidation_0-rmse:920.36660\n",
      "[17]\tvalidation_0-rmse:914.02620\n",
      "[18]\tvalidation_0-rmse:910.37550\n",
      "[19]\tvalidation_0-rmse:907.31251\n",
      "[20]\tvalidation_0-rmse:899.94975\n",
      "[21]\tvalidation_0-rmse:892.48725\n",
      "[22]\tvalidation_0-rmse:886.67234\n",
      "[23]\tvalidation_0-rmse:880.55072\n",
      "[24]\tvalidation_0-rmse:874.20424\n",
      "[25]\tvalidation_0-rmse:869.73386\n",
      "[26]\tvalidation_0-rmse:861.12661\n",
      "[27]\tvalidation_0-rmse:853.29038\n",
      "[28]\tvalidation_0-rmse:847.53957\n",
      "[29]\tvalidation_0-rmse:843.16199\n",
      "[30]\tvalidation_0-rmse:840.74434\n",
      "[31]\tvalidation_0-rmse:832.29534\n",
      "[32]\tvalidation_0-rmse:830.01330\n",
      "[33]\tvalidation_0-rmse:826.38411\n",
      "[34]\tvalidation_0-rmse:822.13216\n",
      "[35]\tvalidation_0-rmse:817.03783\n",
      "[36]\tvalidation_0-rmse:813.70342\n",
      "[37]\tvalidation_0-rmse:811.09470\n",
      "[38]\tvalidation_0-rmse:805.07070\n",
      "[39]\tvalidation_0-rmse:803.00298\n",
      "[40]\tvalidation_0-rmse:797.65919\n",
      "[41]\tvalidation_0-rmse:792.72345\n",
      "[42]\tvalidation_0-rmse:786.69390\n",
      "[43]\tvalidation_0-rmse:781.11742\n",
      "[44]\tvalidation_0-rmse:779.10613\n",
      "[45]\tvalidation_0-rmse:773.56246\n",
      "[46]\tvalidation_0-rmse:768.23903\n",
      "[47]\tvalidation_0-rmse:763.40187\n",
      "[48]\tvalidation_0-rmse:761.42635\n",
      "[49]\tvalidation_0-rmse:758.36563\n",
      "[50]\tvalidation_0-rmse:751.50953\n",
      "[51]\tvalidation_0-rmse:747.03620\n",
      "[52]\tvalidation_0-rmse:743.88773\n",
      "[53]\tvalidation_0-rmse:739.02332\n",
      "[54]\tvalidation_0-rmse:735.63963\n",
      "[55]\tvalidation_0-rmse:733.46773\n",
      "[56]\tvalidation_0-rmse:727.96987\n",
      "[57]\tvalidation_0-rmse:723.98415\n",
      "[58]\tvalidation_0-rmse:720.17231\n",
      "[59]\tvalidation_0-rmse:717.24797\n",
      "[60]\tvalidation_0-rmse:715.88276\n",
      "[61]\tvalidation_0-rmse:712.34340\n",
      "[62]\tvalidation_0-rmse:707.18711\n",
      "[63]\tvalidation_0-rmse:703.78397\n",
      "[64]\tvalidation_0-rmse:699.15570\n",
      "[65]\tvalidation_0-rmse:696.67393\n",
      "[66]\tvalidation_0-rmse:692.72550\n",
      "[67]\tvalidation_0-rmse:689.62869\n",
      "[68]\tvalidation_0-rmse:685.21620\n",
      "[69]\tvalidation_0-rmse:681.56582\n",
      "[70]\tvalidation_0-rmse:678.83986\n",
      "[71]\tvalidation_0-rmse:676.71670\n",
      "[72]\tvalidation_0-rmse:675.76803\n",
      "[73]\tvalidation_0-rmse:674.02343\n",
      "[74]\tvalidation_0-rmse:671.43066\n",
      "[75]\tvalidation_0-rmse:666.48385\n",
      "[76]\tvalidation_0-rmse:663.45174\n",
      "[77]\tvalidation_0-rmse:660.25006\n",
      "[78]\tvalidation_0-rmse:659.00679\n",
      "[79]\tvalidation_0-rmse:658.36797\n",
      "[80]\tvalidation_0-rmse:656.26505\n",
      "[81]\tvalidation_0-rmse:652.83301\n",
      "[82]\tvalidation_0-rmse:649.40749\n",
      "[83]\tvalidation_0-rmse:646.15254\n",
      "[84]\tvalidation_0-rmse:644.32395\n",
      "[85]\tvalidation_0-rmse:641.26346\n",
      "[86]\tvalidation_0-rmse:639.42740\n",
      "[87]\tvalidation_0-rmse:635.21731\n",
      "[88]\tvalidation_0-rmse:632.02543\n",
      "[89]\tvalidation_0-rmse:628.46922\n",
      "[90]\tvalidation_0-rmse:625.31686\n",
      "[91]\tvalidation_0-rmse:622.80299\n",
      "[92]\tvalidation_0-rmse:620.78667\n",
      "[93]\tvalidation_0-rmse:619.13043\n",
      "[94]\tvalidation_0-rmse:617.83489\n",
      "[95]\tvalidation_0-rmse:614.70131\n",
      "[96]\tvalidation_0-rmse:612.14972\n",
      "[97]\tvalidation_0-rmse:609.55972\n",
      "[98]\tvalidation_0-rmse:608.39780\n",
      "[99]\tvalidation_0-rmse:606.12968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treino do Modelo XGB\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(x_train, y_train, eval_set = [(x_train, y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição dos valores \n",
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1185.604463598415"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Métricas do RMSE do modelo\n",
    "rmse = np.sqrt(np.mean((y_test - prediction) ** 2))\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
