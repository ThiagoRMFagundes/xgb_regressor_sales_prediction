{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Read CSV Files\n",
    "# Load the training and testing datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicated rows.\n"
     ]
    }
   ],
   "source": [
    "# ### Check for Duplicates\n",
    "# Concatenate the training and testing datasets to check for duplicates\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Identify duplicated rows in the combined dataset\n",
    "duplicates = combined_data.duplicated()\n",
    "\n",
    "# Check if any duplicates exist and print the results\n",
    "if duplicates.any():\n",
    "    print(\"There are duplicated rows.\")\n",
    "    print(combined_data[duplicates])  # Show the duplicated rows\n",
    "else:\n",
    "    print(\"There are no duplicated rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Handle Missing Values in 'Item_Weight'\n",
    "# The 'Item_Weight' column has missing values that need to be addressed.\n",
    "# Group by 'Item_Identifier' and calculate descriptive statistics to find a suitable replacement for missing values\n",
    "weight_statistics = train_data.groupby('Item_Identifier').agg(\n",
    "    mean=('Item_Weight', 'mean'),\n",
    "    std=('Item_Weight', 'std'),\n",
    "    min=('Item_Weight', 'min'),\n",
    "    q25=('Item_Weight', lambda x: x.quantile(0.25)),\n",
    "    q50=('Item_Weight', 'median'),\n",
    "    q75=('Item_Weight', lambda x: x.quantile(0.75)),\n",
    "    max=('Item_Weight', 'max')\n",
    ")\n",
    "\n",
    "# ### Remove Irrelevant Records\n",
    "# Identify items with a single record that have a null weight\n",
    "# Since there are only 4 such records in over 8k, they will be removed\n",
    "records_to_remove = ['FDN52', 'FDK57', 'FDE52', 'FDQ60']\n",
    "train_data = train_data[~train_data['Item_Identifier'].isin(records_to_remove)]\n",
    "\n",
    "# ### Fill Missing Values with Means\n",
    "# Replace null values in 'Item_Weight' with the mean weight of each item\n",
    "mean_weight_per_item = train_data.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n",
    "train_data['Item_Weight'] = train_data['Item_Weight'].fillna(mean_weight_per_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Standardize 'Item_Fat_Content' Categories\n",
    "# Unify categories that represent the same concept\n",
    "category_replacements = {\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat',\n",
    "    'reg': 'Regular'\n",
    "}\n",
    "train_data['Item_Fat_Content'] = train_data['Item_Fat_Content'].replace(category_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Consolidate Rare Categories in 'Item_Type'\n",
    "# Replace categories with fewer than 200 records as they may be underrepresented\n",
    "train_data.loc[train_data['Item_Type'].isin(['Starchy Foods', 'Breakfast', 'Seafood']), 'Item_Type'] = 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existem items com visibilidade 0, parece extranho porém não irei editar. \n",
    "# O item pode não ficar exposto na loja, além disso esses itens são únicos por outlet então não teriam como substituir por alguma outra métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alguns Outlets não tem o valor do tamanho, ficará nulo mesmo pois não temos como saber o tamanho de fato do Outlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_data = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronização dos dados, apesar do XGB ser robusto com escalas diferentes, pretendo realizar uma analise de importancia das categorias e um tunning do modelo.\n",
    "# A padronização pode ser benéfica para essas análises futuras. \n",
    "# Além disso, a utilizarei a Padronização pois ela é menos sensivel a outlier do que a Normalização\n",
    "scaler = StandardScaler()\n",
    "treated_train_data[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']] = scaler.fit_transform(\n",
    "    treated_train_data[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding nas colunas categoricas pois o XGBRegressor não aceita colunas categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding na coluna de Gordura e Tamanho do outlet, visto que tem apenas 2 valores então essa binaridade será capturada\n",
    "le_fat = LabelEncoder()\n",
    "treated_train_data['Item_Fat_Content'] = le_fat.fit_transform(treated_train_data['Item_Fat_Content'])\n",
    "treated_train_data['Outlet_Size'] = le_fat.fit_transform(treated_train_data['Outlet_Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding no Item_Type pois tem muitas categorias e quero evitar induzir uma ordem/hierarquia nas categorias. O xgb funciona bem com entradas binárias\n",
    "treated_train_data = pd.get_dummies(treated_train_data, columns=['Item_Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding na coluna Tier, apenas removi o \"Tier \" do nome da categoria\n",
    "treated_train_data['Outlet_Location_Type'] = treated_train_data['Outlet_Location_Type'].str.replace('Tier ', '', regex=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding no Outlet_Type pois tem muitas categorias e quero evitar induzir uma ordem/hierarquia nas categorias. O xgb funciona bem com entradas binárias\n",
    "treated_train_data = pd.get_dummies(treated_train_data, columns=['Outlet_Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as categoria previsoras da que será prevista. \n",
    "# Removendo Identificadores pois geralmente não tem relação direta com as vendas e removendo a coluna q será prevista\n",
    "x_train_data = treated_train_data.drop(['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales'], axis = 1)\n",
    "# Isolando a coluna q será prevista\n",
    "y_train_data = treated_train_data['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1413.11852\n",
      "[1]\tvalidation_0-rmse:1239.61964\n",
      "[2]\tvalidation_0-rmse:1142.26115\n",
      "[3]\tvalidation_0-rmse:1086.48732\n",
      "[4]\tvalidation_0-rmse:1054.40376\n",
      "[5]\tvalidation_0-rmse:1034.65314\n",
      "[6]\tvalidation_0-rmse:1019.07278\n",
      "[7]\tvalidation_0-rmse:1008.17237\n",
      "[8]\tvalidation_0-rmse:997.78937\n",
      "[9]\tvalidation_0-rmse:991.80089\n",
      "[10]\tvalidation_0-rmse:980.81496\n",
      "[11]\tvalidation_0-rmse:973.90219\n",
      "[12]\tvalidation_0-rmse:966.35763\n",
      "[13]\tvalidation_0-rmse:960.52228\n",
      "[14]\tvalidation_0-rmse:951.76886\n",
      "[15]\tvalidation_0-rmse:946.96109\n",
      "[16]\tvalidation_0-rmse:939.36103\n",
      "[17]\tvalidation_0-rmse:937.26896\n",
      "[18]\tvalidation_0-rmse:930.75491\n",
      "[19]\tvalidation_0-rmse:925.09663\n",
      "[20]\tvalidation_0-rmse:920.97174\n",
      "[21]\tvalidation_0-rmse:915.32086\n",
      "[22]\tvalidation_0-rmse:909.27095\n",
      "[23]\tvalidation_0-rmse:904.99484\n",
      "[24]\tvalidation_0-rmse:900.12461\n",
      "[25]\tvalidation_0-rmse:894.68683\n",
      "[26]\tvalidation_0-rmse:888.74248\n",
      "[27]\tvalidation_0-rmse:884.16218\n",
      "[28]\tvalidation_0-rmse:878.91794\n",
      "[29]\tvalidation_0-rmse:873.11050\n",
      "[30]\tvalidation_0-rmse:868.24638\n",
      "[31]\tvalidation_0-rmse:863.26253\n",
      "[32]\tvalidation_0-rmse:858.60414\n",
      "[33]\tvalidation_0-rmse:855.77444\n",
      "[34]\tvalidation_0-rmse:851.47617\n",
      "[35]\tvalidation_0-rmse:847.89030\n",
      "[36]\tvalidation_0-rmse:843.59615\n",
      "[37]\tvalidation_0-rmse:839.54944\n",
      "[38]\tvalidation_0-rmse:835.54290\n",
      "[39]\tvalidation_0-rmse:830.05666\n",
      "[40]\tvalidation_0-rmse:826.42559\n",
      "[41]\tvalidation_0-rmse:821.84812\n",
      "[42]\tvalidation_0-rmse:818.40379\n",
      "[43]\tvalidation_0-rmse:814.52928\n",
      "[44]\tvalidation_0-rmse:810.90147\n",
      "[45]\tvalidation_0-rmse:806.86771\n",
      "[46]\tvalidation_0-rmse:801.74690\n",
      "[47]\tvalidation_0-rmse:797.52161\n",
      "[48]\tvalidation_0-rmse:794.39038\n",
      "[49]\tvalidation_0-rmse:790.02097\n",
      "[50]\tvalidation_0-rmse:784.57382\n",
      "[51]\tvalidation_0-rmse:779.65840\n",
      "[52]\tvalidation_0-rmse:776.51811\n",
      "[53]\tvalidation_0-rmse:772.96765\n",
      "[54]\tvalidation_0-rmse:769.66015\n",
      "[55]\tvalidation_0-rmse:765.65898\n",
      "[56]\tvalidation_0-rmse:762.75042\n",
      "[57]\tvalidation_0-rmse:761.34262\n",
      "[58]\tvalidation_0-rmse:757.89694\n",
      "[59]\tvalidation_0-rmse:755.69395\n",
      "[60]\tvalidation_0-rmse:754.52158\n",
      "[61]\tvalidation_0-rmse:752.88203\n",
      "[62]\tvalidation_0-rmse:749.86938\n",
      "[63]\tvalidation_0-rmse:747.68196\n",
      "[64]\tvalidation_0-rmse:745.09479\n",
      "[65]\tvalidation_0-rmse:743.94489\n",
      "[66]\tvalidation_0-rmse:740.08885\n",
      "[67]\tvalidation_0-rmse:737.43765\n",
      "[68]\tvalidation_0-rmse:733.26825\n",
      "[69]\tvalidation_0-rmse:729.88609\n",
      "[70]\tvalidation_0-rmse:727.03987\n",
      "[71]\tvalidation_0-rmse:724.06216\n",
      "[72]\tvalidation_0-rmse:722.23298\n",
      "[73]\tvalidation_0-rmse:718.81707\n",
      "[74]\tvalidation_0-rmse:716.53760\n",
      "[75]\tvalidation_0-rmse:713.72529\n",
      "[76]\tvalidation_0-rmse:709.91936\n",
      "[77]\tvalidation_0-rmse:706.14523\n",
      "[78]\tvalidation_0-rmse:705.03730\n",
      "[79]\tvalidation_0-rmse:703.09538\n",
      "[80]\tvalidation_0-rmse:699.24769\n",
      "[81]\tvalidation_0-rmse:696.15886\n",
      "[82]\tvalidation_0-rmse:693.74250\n",
      "[83]\tvalidation_0-rmse:691.34588\n",
      "[84]\tvalidation_0-rmse:689.01136\n",
      "[85]\tvalidation_0-rmse:686.33686\n",
      "[86]\tvalidation_0-rmse:683.38950\n",
      "[87]\tvalidation_0-rmse:682.13736\n",
      "[88]\tvalidation_0-rmse:679.97744\n",
      "[89]\tvalidation_0-rmse:677.73390\n",
      "[90]\tvalidation_0-rmse:675.14037\n",
      "[91]\tvalidation_0-rmse:672.62848\n",
      "[92]\tvalidation_0-rmse:670.72263\n",
      "[93]\tvalidation_0-rmse:669.13661\n",
      "[94]\tvalidation_0-rmse:666.91498\n",
      "[95]\tvalidation_0-rmse:663.54061\n",
      "[96]\tvalidation_0-rmse:661.86398\n",
      "[97]\tvalidation_0-rmse:658.42460\n",
      "[98]\tvalidation_0-rmse:654.79760\n",
      "[99]\tvalidation_0-rmse:651.02728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treino do Modelo XGB\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(x_train_data, y_train_data, eval_set = [(x_train_data, y_train_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição dos valores \n",
    "prediction = model.predict(x_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651.0272790963623"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Métricas do RMSE do modelo\n",
    "rmse = np.sqrt(np.mean((y_train_data - prediction) ** 2))\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
